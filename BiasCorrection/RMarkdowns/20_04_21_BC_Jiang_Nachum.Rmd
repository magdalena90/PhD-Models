---
title: 'Bias Correction using the Weighting Technique'
output:
  html_document:
    code_folding: 'hide'
---

<br>

### Problem
<br>

As it can be seen in 20_04_08_Ridge.html, there is a relation between the score the model assigns to a gene and the gene's mean level of expression. This is a problem because we had previously discovered a bias in the SFARI scores related to mean level of expression (Preprocessing/Gandal/AllRegions/RMarkdowns/20_04_03_SFARI_genes.html), which means that this could be a confounding factor in our model and the reason why it seems to perform well, so we need to remove this bias to recover the true biological signal that is mixed with it and improve the quality of our model.

<br>

### Weighting Technique
<br>

Description of the algorithm


```{r load_packages, warning=FALSE, message=FALSE}

library(tidyverse)
library(knitr)
library(plotly) ; library(viridis) ; library(gridExtra) ; library(RColorBrewer) ; library(corrplot)
library(biomaRt)
library(Rtsne)
library(caret) ; library(ROCR) ; library(car)
library(polycor)
library(expss) ; library(knitr)

SFARI_colour_hue = function(r) {
  pal = c('#FF7631','#FFB100','#E8E328','#8CC83F','#62CCA6','#59B9C9','#b3b3b3','#808080','gray','#d9d9d9')[r]
}
```

#### Load data
```{r, warning=FALSE, message=FALSE}
# Clusterings
clustering_selected = 'DynamicHybridMergedSmall'
clusterings = read_csv('./../Data/clusters.csv')
clusterings$Module = clusterings[,clustering_selected] %>% data.frame %>% unlist %>% unname

assigned_module = data.frame('ID' = clusterings$ID, 'Module' = clusterings$Module)

# Original dataset
original_dataset = read.csv(paste0('./../Data/dataset_', clustering_selected, '.csv'), row.names=1)

# Model dataset
load('./../Data/LR_model.RData')

# Regression data
load(file='./../Data/Ridge_model.RData')

# Mean Expression data
load('./../Data/preprocessed_data.RData')
datExpr = datExpr %>% data.frame
DE_info = DE_info %>% data.frame

# Dataset created with DynamicTreeMerged algorithm
clustering_selected = 'DynamicHybridMergedSmall'
original_dataset = read.csv(paste0('./../Data/dataset_', clustering_selected, '.csv'), row.names=1)

# Add gene symbol
getinfo = c('ensembl_gene_id','external_gene_id')
mart = useMart(biomart='ENSEMBL_MART_ENSEMBL', dataset='hsapiens_gene_ensembl',
               host='feb2014.archive.ensembl.org')
gene_names = getBM(attributes=getinfo, filters=c('ensembl_gene_id'), values=rownames(dataset), mart=mart)

rm(dds, datGenes, datMeta, clustering_selected, clusterings, mart, getinfo)
```
<br>

## Remove Bias
<br>

### Demographic Parity
<br>

Using **Demographic Parity** as a measure of bias: A fair classifier *h* should make positive predictions each segment $G$ of the population at the same rate as in all of the population

This definition is for discrete segments of the population. Since our bias is found across all the population but in different measures depending on the mean level of expression of the gene, we have to adapt this definition to a continuous bias scenario

**Demographic Parity for our problem:** A fair classifier *h* should make positive predictions on genes with a certail mean level of expression as on all of the genes in the dataset

<br>

#### Demographic Parity bias metric
<br>

The original formula for the Demographic Parity bias is

- $c(x,0) = 0 $ when the prediction is negative

- $c(x,1) = \frac{g(x)}{Z_G}-1$ when the prediction is positive. Where $g(x)$ is the Kronecker delta to indicate if the sample belongs to the protected group and $Z_G$ is the proportion of the population that belongs to the group we want to protect against bias

Using this definitions in our problem:

$g(x):$ Since all our samples belong to the protected group, this would always be 1

$Z_G:$ Since all of our samples belong to the protected group, this would also always be 1

So our measure of bias $c(x,1) = \frac{1}{1}-1 = 0$ for all samples. We need to adapt it to our continous case

<br>

#### Adaptation of the bias metric
<br>




```{r, warning=FALSE, message=FALSE}

train_set$SFARI = train_set$SFARI %>% as.factor

set.seed(123)

# Initial parameters
eta = 1
Loops = 50
lambda = 0
w = rep(1, nrow(train_set))
h = train(SFARI ~., data = train_set, method = 'glmnet', trControl = trainControl('cv', number = 10),
              tuneGrid = expand.grid(alpha = 0, lambda = 10^seq(1, -3, by = -.1)))

mean_expr = data.frame('ID' = rownames(datExpr), 'meanExpr' = rowMeans(datExpr)) %>%
            filter(ID %in% rownames(train_set)) %>% right_join(data.frame('ID' = substr(rownames(train_set),1,15)), by = 'ID') %>%
            mutate('meanExpr_norm' = (meanExpr-min(meanExpr))/(max(meanExpr)-min(meanExpr))-0.5)

# Track behaviour of plot
bias_vec = c()
acc_vec = c()

for(l in 1:Loops){
  
  # Calculate bias
  bias = hetcor(predict(h,train_set) %>% as.numeric, mean_expr$meanExpr)$correlations[1,2]
  
  # Update weights
  lambda = lambda - eta*bias
  w_hat = exp(lambda*mean_expr$meanExpr_norm)
  w = 1/(1+w_hat)
  w[train_set$SFARI %>% as.logical] = w[train_set$SFARI %>% as.logical] * w_hat[train_set$SFARI %>% as.logical]
  
  # Update tracking vars
  bias_vec = c(bias_vec, bias)
  acc_vec = c(acc_vec, mean(predict(h,train_set) == train_set$SFARI))
  
  # Update h
  h = train(SFARI ~., data = train_set, method = 'glmnet', weights = w, trControl = trainControl('cv', number = 10),
              tuneGrid = expand.grid(alpha = 0, lambda = 10^seq(1, -3, by = -.1)))
}

plot_info = data.frame('iter' = 1:length(bias_vec), 'bias' = bias_vec, 'accuracy' = acc_vec) %>% melt(id.vars = 'iter')

plot_info %>% ggplot(aes(x=iter, y=value, color = variable)) + geom_line() + theme_minimal()


rm(eta, Loops, l, w_hat, bias)
```


```{r}

w_hat = exp(lambda*mean_expr$meanExpr_norm) # inverso a mean expr
w0 = 1/(1+w_hat) # prop a mean expr
w = 1/(1+w_hat)
w[train_set$SFARI %>% as.logical] = w[train_set$SFARI %>% as.logical]*w_hat[train_set$SFARI %>% as.logical] # inv mean expr Positives, prop Negatives
plot_data = data.frame(meanExpr = mean_expr$meanExpr, w_hat = w_hat, w0 = w0, w = w, SFARI = train_set$SFARI, pred = predict(h,train_set))

plot_data %>% ggplot(aes(meanExpr, w, color = SFARI)) + geom_point(alpha = 0.3) + ylab('weight') + xlab('Mean Expression') + 
              ggtitle('Final weights of the model') + ylim(c(0,1)) + theme_minimal()

# lambdas = seq(0,2*lambda,10)
# accs = sapply(lambdas, function(l) {
# })

```

<br>

#### Results
<br>

The linear part of the bias is gone, the problem is that the non-linear part is sthill there (although weak)

This is because we are measuring bias with correlation, which is measures linear relations, if we can come up with another non-linear metric to measure bias, we may be able to get rid of the non-linear part of the bias as well as the linear

```{r, warning=FALSE, message=FALSE}

# Correct Bias
predictions = h %>% predict(test_set, type='prob')
test_set$corrected_score = predictions$`TRUE`
test_set$corrected_pred = test_set$corrected_score>0.5

# Plot results
plot_data = data.frame('ID'=rownames(datExpr), 'meanExpr'=rowMeans(datExpr)) %>% 
            right_join(test_set %>% mutate(ID=rownames(test_set)), by='ID')

plot_data %>% ggplot(aes(meanExpr, corrected_score)) + geom_point(alpha=0.1, color='#0099cc') + 
              geom_smooth(method='lm', color='#999999', se = FALSE, alpha=0.6) +
              geom_smooth(method='gam', color='gray', alpha=0.2) + ylab('Corrected Score') + xlab('Mean Expression') +
              theme_minimal() + ggtitle('Mean expression vs Model score corrected using adjusted weights')
```
<br>

---

<br>

## Performance Metrics
<br>

#### Confusion matrix
```{r}
conf_mat = test_set %>% apply_labels(SFARI = 'Actual Labels', 
                                     corrected_score = 'Corrected Score', 
                                     corrected_pred = 'Corrected Label Prediction')

cro(conf_mat$SFARI, list(conf_mat$corrected_pred, total()))

rm(conf_mat)
```

#### Accuracy
<br>

The accuracy was expected to decrease because the bias was helping classify samples correctly, but for the wrong reasons

The accuracy does decrease a lot, but it seems to be because the model is biased towards classifying samples as positive

```{r}
old_acc = mean(test_set$SFARI==(test_set$prob>0.5))
acc = mean(test_set$SFARI==(test_set$corrected_pred))

cat(paste0('Accuracy = ', round(acc,4)))
cat(paste0('Accuracy decreased ',round(old_acc-acc,4), ' points'))


rm(acc, old_acc)
```

#### ROC Curve
<br>


```{r}
pred_ROCR = prediction(test_set$corrected_score, test_set$SFARI)

roc_ROCR = performance(pred_ROCR, measure='tpr', x.measure='fpr')
AUC = performance(pred_ROCR, measure='auc')@y.values[[1]]

plot(roc_ROCR, main=paste0('ROC curve (AUC=',round(AUC,2),')'), col='#009999')
abline(a=0, b=1, col='#666666')

rm(roc_ROCR, AUC)
```

#### Lift Curve
<br>

Lift decreased from a starting point of almost 20 to just 5. The genes most affected seem to have been the ones with the highest scores
```{r}
lift_ROCR = performance(pred_ROCR, measure='lift', x.measure='rpp')
plot(lift_ROCR, main='Lift curve', col='#86b300')

rm(lift_ROCR, pred_ROCR)
```


---

## Analyse Model

Looks very similar to before, the means of each group are a bit closer together

```{r}
plot_data = test_set %>% dplyr::select(corrected_score, SFARI)

ggplotly(plot_data %>% ggplot(aes(corrected_score, fill=SFARI, color=SFARI)) + geom_density(alpha=0.3) + xlab('Score') +
         geom_vline(xintercept = mean(plot_data$corrected_score[plot_data$SFARI]), color = '#00C0C2', linetype = 'dashed') +
         geom_vline(xintercept = mean(plot_data$corrected_score[!plot_data$SFARI]), color = '#FF7371', linetype = 'dashed') +
         theme_minimal() + ggtitle('Model score distribution by SFARI Label'))
```

The positive relation between SFARI scores and Model scores is still there but is not as strong as before
```{r, fig.width=10}
plot_data = test_set %>% mutate(ID=rownames(test_set)) %>% dplyr::select(ID, corrected_score) %>%
            left_join(original_dataset %>% mutate(ID=rownames(original_dataset)), by='ID') %>%
            dplyr::select(ID, corrected_score, gene.score) %>% apply_labels(gene.score='SFARI Gene score')

cro(plot_data$gene.score)

ggplotly(plot_data %>% ggplot(aes(gene.score, corrected_score, fill=gene.score)) + geom_boxplot() + 
              scale_fill_manual(values=SFARI_colour_hue(r=c(1:6,8,7))) + 
              ggtitle('Distribution of the Model scores by SFARI score') +
              xlab('SFARI score') + ylab('Model score') + theme_minimal())
```

Print genes with highest corrected scores in test set

- We lost all the genes with a score of 1, so they probably were in the top because of their high level of expression

- Three SFARI genes remain (all with score 3), so they still have a higher concentration (1:17) than in the whole test set (1:69)

```{r}
test_set %>% dplyr::select(corrected_score, SFARI) %>% mutate(ID = rownames(test_set)) %>% 
             arrange(desc(corrected_score)) %>% top_n(50, wt=corrected_score) %>%
             left_join(original_dataset %>% mutate(ID=rownames(original_dataset)), by='ID')  %>% 
             left_join(gene_names, by = c('ID'='ensembl_gene_id')) %>%
             dplyr::rename('GeneSymbol' = external_gene_id, 'Probability' = corrected_score,
                           'ModuleDiagnosis_corr' = MTcor, 'GeneSignificance' = GS) %>%
             mutate(ModuleDiagnosis_corr = round(ModuleDiagnosis_corr,4), Probability = round(Probability,4), 
                    GeneSignificance = round(GeneSignificance,4)) %>%
             dplyr::select(GeneSymbol, GeneSignificance, ModuleDiagnosis_corr, Module, Probability, gene.score) %>%
             kable(caption = 'Genes with highest model probabilities from the test set')
```

---

<br><br>

### Negative samples distribution
<br>

- The genes with the lowest scores were affected the most as a group

```{r, warning=FALSE, message=FALSE, fig.height=8}
negative_set = test_set %>% filter(!SFARI)

negative_set %>% mutate(diff = abs(prob-corrected_score)) %>% 
             ggplot(aes(prob, corrected_score, color = diff)) + geom_point(alpha=0.2) + scale_color_viridis() + 
             geom_abline(slope=1, intercept=0, color='gray', linetype='dashed') + 
             geom_smooth(color='#666666', alpha=0.5, se=TRUE, size=0.5) + coord_fixed() +
             xlab('Original probability') + ylab('Corrected probability') + theme_minimal() + theme(legend.position = 'none')
```

```{r}
negative_set_table = negative_set %>% mutate(corrected_pred = corrected_score>0.5) %>%
                     apply_labels(corrected_score = 'Corrected Probability', 
                                  corrected_pred = 'Corrected Class Prediction',
                                  pred = 'Original Class Prediction')

cro(negative_set_table$pred, list(negative_set_table$corrected_pred, total()))

cat(paste0('\n', round(100*mean(negative_set_table$corrected_pred == negative_set_table$pred)),
           '% of the genes maintained their original predicted class'))

rm(negative_set_table)
```

#### Probability and Gene Significance
<br>

The relation is not as strong as before in the highest scores

*The transparent verison of the trend line is the original trend line

```{r, message=FALSE, warning=FALSE}
negative_set %>% ggplot(aes(corrected_score, GS, color=MTcor)) + geom_point() + geom_smooth(method='gam', color='#666666') +
                 geom_line(stat='smooth', method='gam', color='#666666', alpha=0.5, size=1.2, aes(x=prob)) +
                 geom_hline(yintercept=mean(negative_set$GS), color='gray', linetype='dashed') +
                 scale_color_gradientn(colours=c('#F8766D','white','#00BFC4')) + xlab('Corrected Score') +
                 ggtitle('Relation between the Model\'s Corrected Score and Gene Significance') + theme_minimal()
```

Summarised version of score vs mean expression, plotting by module instead of by gene

The difference in the trend lines between this plot and the one above is that the one above takes all the points into consideration while this considers each module as an observation by itself, so the top one is strongly affected by big modules and the bottom one treats all modules the same

The transparent version of each point and trend lines are the original values and trends before the bias correction

- The average score for the modules with negative correlation decreased a little and for the ones with positive correlation increased a little
```{r, warning=FALSE, message=FALSE}
plot_data = negative_set %>% group_by(MTcor) %>% summarise(mean = mean(prob), sd = sd(prob),new_mean = mean(corrected_score),
                                                           new_sd = sd(corrected_score), n = n()) %>%
            mutate(MTcor_sign = ifelse(MTcor>0, 'Positive', 'Negative')) %>% left_join(original_dataset, by='MTcor') %>%
            dplyr::select(Module, MTcor, MTcor_sign, mean, new_mean, sd, new_sd, n) %>% distinct()
colnames(plot_data)[1] = 'ID'

ggplotly(plot_data %>% ggplot(aes(MTcor, new_mean, size=n, color=MTcor_sign)) + geom_point(aes(id = ID)) + 
         geom_smooth(method='loess', color='gray', se=FALSE) + geom_smooth(method='lm', se=FALSE) + 
         geom_point(aes(y=mean), alpha=0.3) + xlab('Module-Diagnosis correlation') + ylab('Mean Corrected Score by the Model') + 
         geom_line(stat='smooth', method='loess', color='gray', se=FALSE, alpha=0.3, size=1.2, aes(y=mean)) + 
         geom_line(stat='smooth', method='lm', se=FALSE, alpha=0.3, size=1.2, aes(y=mean)) + 
         theme_minimal() + theme(legend.position='none'))
```
<br>

#### Probability and mean level of expression
<br>

Check if correcting by gene also corrected by module: Yes, but not enough to remove the bias completely

```{r, warning=FALSE, message=FALSE, fig.width=10}
mean_and_sd = data.frame(ID=rownames(datExpr), meanExpr=rowMeans(datExpr), sdExpr=apply(datExpr,1,sd))

plot_data = negative_set %>% mutate(ID=rownames(test_set)[!test_set$SFARI]) %>% left_join(mean_and_sd, by='ID') %>% 
            left_join(original_dataset %>% mutate(ID=rownames(original_dataset)) %>% 
                      dplyr::select(ID, Module), by='ID')

plot_data2 = plot_data %>% group_by(Module) %>% summarise(meanExpr = mean(meanExpr), meanProb = mean(prob), 
                                                          new_meanProb = mean(corrected_score), n=n())

ggplotly(plot_data2 %>% ggplot(aes(meanExpr, new_meanProb, size=n)) + 
         geom_point(color=plot_data2$Module) + geom_point(color=plot_data2$Module, alpha=0.3, aes(y=meanProb)) + 
         geom_smooth(method='loess', se=TRUE, color='gray', alpha=0.1, size=0.7) + 
         geom_line(stat='smooth', method='loess', se=TRUE, color='gray', alpha=0.4, size=1.2, aes(y=meanProb)) +
         theme_minimal() + theme(legend.position='none') + xlab('Mean Expression') + ylab('Corrected Probability') +
         ggtitle('Mean expression vs corrected Model score by Module'))

rm(plot_data2, mean_and_sd)
```
<br>

#### Probability and SD of level of expression
<br>

The relation between SD and score became bigger than before

```{r, warning=FALSE, message=FALSE}
plot_data %>% ggplot(aes(sdExpr, corrected_score)) + geom_point(alpha=0.1, color='#0099cc') + 
              geom_smooth(method='lm', color='#999999', se=FALSE, alpha=1) + xlab('SD') + ylab('Corrected Probability') +
              geom_line(stat='smooth', method='lm', color='#999999', se=FALSE, alpha=0.4, size=1.5, aes(y=prob)) + 
              theme_minimal() + ggtitle('SD vs model probability by gene') + scale_x_sqrt()
```

<!-- This fitted curve looks like the opposite of the trend found between mean/sd and model scores -->
<!-- ```{r, message=FALSE, warning=FALSE} -->
<!-- plot_data %>% ggplot(aes(meanExpr, sdExpr)) + geom_point(alpha=0.1, color='#0099cc') +  -->
<!--               geom_smooth(method='loess', color='gray', alpha=0.3) +  -->
<!--               geom_smooth(method='lm', color='#999999', se=FALSE, alpha=1) +  -->
<!--               scale_x_log10() + scale_y_log10() +  xlab('Mean Expression') + ylab('SD of Expression') + -->
<!--               theme_minimal() + ggtitle('Mean expression vs SD by gene') -->
<!-- ``` -->
<br>

#### Probability and lfc
<br>

The relation seems to have gotten a bit stronger for the over-expressed genes and a bit weaker for the under-expressed genes
```{r, fig.width=10, message=FALSE, warning=FALSE}
plot_data = negative_set %>% mutate(ID=rownames(test_set)[!test_set$SFARI]) %>% 
            left_join(DE_info %>% mutate(ID=rownames(DE_info)), by='ID')

plot_data %>% ggplot(aes(log2FoldChange, corrected_score)) + geom_point(alpha=0.1, color='#0099cc') + 
              geom_smooth(method='loess', color='gray', alpha=0.1) + xlab('LFC') + ylab('Corrected Probability') +
              geom_line(stat='smooth', method='loess', color='gray', alpha=0.4, size=1.5, aes(y=prob)) +
              theme_minimal() + ggtitle('LFC vs model probability by gene')
```



```{r, fig.width=10, message=FALSE, warning=FALSE}
plot_data %>% mutate(DE = padj<0.05) %>% ggplot(aes(log2FoldChange, corrected_score, color=DE)) + geom_point(alpha=0.1) + 
             geom_smooth(method='loess', alpha=0.1) + xlab('LFC') + ylab('Corrected Probability') + 
              geom_line(stat='smooth', method='loess', alpha=0.4, size=1.5, aes(y=prob, color = DE)) +
              theme_minimal() + ggtitle('LFC vs model probability by gene')
```
<br>

#### Probability and Module-Diagnosis correlation
<br>

Not much change

```{r warning=FALSE, message=FALSE, fig.width=10}

module_score = negative_set %>% mutate(ID=rownames(test_set)[!test_set$SFARI]) %>%
               left_join(original_dataset %>% mutate(ID = rownames(original_dataset)), by='ID') %>%
               dplyr::select(ID, prob, corrected_score, Module, MTcor.x) %>% rename(MTcor = MTcor.x) %>% 
               left_join(data.frame(MTcor=unique(dataset$MTcor)) %>% arrange(by=MTcor) %>% 
                         mutate(order=1:length(unique(dataset$MTcor))), by='MTcor')

ggplotly(module_score %>% ggplot(aes(MTcor, corrected_score)) + geom_point(color=module_score$Module, aes(id=ID, alpha=corrected_score^4)) +
         geom_hline(yintercept=mean(module_score$corrected_score), color='gray', linetype='dotted') + 
         geom_line(stat='smooth', method = 'loess', color='gray', alpha=0.5, size=1.5, aes(x=MTcor, y=prob)) +
         geom_smooth(color='gray', method = 'loess', se = FALSE, alpha=0.3) + theme_minimal() + 
         xlab('Module-Diagnosis correlation') + ylab('Corrected Score'))

```

<br><br>

### Conclusion
<br>

This bias correction seems to be working but we may be losing a bit of biological signal on the way, mainly for under-expressed genes

---

#### Saving results

```{r save_results}
write.csv(dataset, file='./../Data/BC_weighting_approach.csv', row.names = TRUE)
```
<br><br>

---

#### Session info

```{r print_session_info}
sessionInfo()
```
<br><br>
